{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech Filtering\n",
    "\n",
    "Using Tf-Idf and BoW to build SVM model\n",
    "\n",
    "Linear model? Or RBF model?\n",
    "\n",
    "RACIST texts classified with '1'\n",
    "NON-RACIST texts classified with '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Web scraper libraries\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# ML libraries\n",
    "import re \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Logistic Regression\n",
    "lreg = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 100.0.4896\n",
      "Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_mac64_m1.zip\n",
      "Driver has been saved in cache [/Users/tonycui/.wdm/drivers/chromedriver/mac64_m1/100.0.4896.60]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.splcenter.org/fighting-hate/extremist-files/group/stormfront\n",
      "WEB SCRAPING FINISHED\n"
     ]
    }
   ],
   "source": [
    "def get_page_links(pageDriver, baseUrl):\n",
    "    a_tags = driver.find_elements_by_tag_name('a')\n",
    "\n",
    "    # Get all urls\n",
    "    urls = [tag.get_attribute('href') for tag in a_tags]\n",
    "\n",
    "    #Get urls that start with baseUrl\n",
    "    useful_urls = [url for url in urls if url and baseUrl in url]\n",
    "\n",
    "    return useful_urls\n",
    "\n",
    "def get_page_text(pageDriver):\n",
    "    pageBody = pageDriver.find_element_by_xpath(\"/html/body\")\n",
    "    if not pageBody:\n",
    "        return \"\"\n",
    "    return pageBody.text\n",
    "\n",
    "\n",
    "visited_urls = set()\n",
    "unvisited_urls = set()\n",
    "error_urls = set()\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument('--log-level=1')\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "baseUrl = input(\"Please enter url: \")\n",
    "unvisited_urls.add(baseUrl)\n",
    "\n",
    "ct = 0\n",
    "while unvisited_urls:\n",
    "    # Remove url from unvisited and add it to visited\n",
    "    currPage = unvisited_urls.pop()\n",
    "    print(currPage)\n",
    "    visited_urls.add(currPage)\n",
    "\n",
    "    # Get page content\n",
    "    try:\n",
    "        driver.get(currPage)\n",
    "    except:\n",
    "        error_urls.add(currPage)\n",
    "        print('Error getting page: ', currPage)\n",
    "        continue\n",
    "\n",
    "    # Get links\n",
    "    try:\n",
    "        currLinks = get_page_links(driver, baseUrl)\n",
    "    except:\n",
    "        error_urls.add(currPage)\n",
    "        print('Error getting links: ', currPage)\n",
    "        continue\n",
    "\n",
    "    # Get text\n",
    "    try:\n",
    "        currText = get_page_text(driver)\n",
    "        textToWrite = currText.splitlines()\n",
    "        with open(f'testCSV.csv', 'w') as csvfile:\n",
    "            fieldnames = ['id', 'text']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            for text in textToWrite:\n",
    "                writer.writerow({'id': ct, 'text': {text}})\n",
    "                ct += 1\n",
    "    except:\n",
    "        error_urls.add(currPage)\n",
    "        print('Error getting text: ', currPage)\n",
    "        continue\n",
    "\n",
    "print(\"WEB SCRAPING FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data pandas, keep raw copy of data\n",
    "train_RAW = pd.read_csv('train.csv')\n",
    "test_RAW = pd.read_csv('testCSV.csv')\n",
    "train = train_RAW\n",
    "test = test_RAW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "Ref: \n",
    "    https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20(term%20frequency%2D,across%20a%20set%20of%20documents\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF freq\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words is a Natural Language Processing technique of text modelling. \n",
    "In technical terms, we can say that it is a method of feature extraction with text data.\n",
    "\n",
    "Ref: \n",
    "    https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words Model\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "Ref/Doc:\n",
    "    https://scikit-learn.org/stable/modules/svm.html#:~:text=Support%20vector%20machines%20(SVMs)%20are,than%20the%20number%20of%20samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/sry46qbs59n25vtlyr3vfqrh0000gn/T/ipykernel_61670/4241641124.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  allTexts = train.append(test, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32034</th>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Hatewatch'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32035</th>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Intelligence Report'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32036</th>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Publications'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32037</th>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Law Enforcement'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32038</th>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Activist Toolkits'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  label                     text\n",
       "32034  72    NaN            {'Hatewatch'}\n",
       "32035  73    NaN  {'Intelligence Report'}\n",
       "32036  74    NaN         {'Publications'}\n",
       "32037  75    NaN      {'Law Enforcement'}\n",
       "32038  76    NaN    {'Activist Toolkits'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine training data with test data\n",
    "allTexts = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "allTexts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text, pattern):\n",
    "    process = re.findall(pattern, text)\n",
    "    for i in process:\n",
    "        text = re.sub(i, '', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text \n",
    "# Add new column 'tidy_tweet'\n",
    "\n",
    "# Clean user handles (@user_name)\n",
    "allTexts['tidy_text'] = np.vectorize(clean_texts)(\n",
    "    allTexts['text'], \"@[\\w]*\") \n",
    "\n",
    "# Clean non-alphabets\n",
    "allTexts['tidy_text'] = allTexts['tidy_text'].str.replace(\n",
    "    \"[^a-zA-Z]\", \" \") \n",
    "\n",
    "allTexts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenized_text = allTexts['tidy_text'].apply(lambda x: x.split())\n",
    "\n",
    "tokenized_text.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenized_text.apply(\n",
    "    lambda x: [lemmatizer.lemmatize(i) for i in x])  \n",
    "\n",
    "# Combine tokens back together\n",
    "for i in range(len(tokenized_text)):\n",
    "    tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "\n",
    "# Replace with lemmatized\n",
    "allTexts['tidy_text'] = tokenized_text\n",
    "\n",
    "allTexts.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction - Bag-of-Words [sklearn’s CountVectorizer]\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(allTexts['tidy_text'])\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Features - Looks at frequency of occurence for terms/importance of the term\n",
    "\n",
    "# TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "# IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "# \n",
    "# TF-IDF = TF*IDF\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(allTexts['tidy_text'])\n",
    "\n",
    "print(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:31962, :]\n",
    "test_bow = bow[31962:, :]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "\n",
    "\n",
    "\n",
    "# xtrain_bow training dataset\n",
    "# xvalid_bow validation for training\n",
    "# ytrain label vector\n",
    "# yvalid validation label vector\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(\n",
    "    train_bow, train['label'], test_size=0.1)\n",
    "\n",
    "# train_bow feature matrix\n",
    "# train['label'] label vector\n",
    "# random_state shuffles data before split into training and testing\n",
    "# test_size percentage of data gets tested on (0.9 training)\n",
    "\n",
    "\n",
    "# print(xtrain_bow)\n",
    "# print(xvalid_bow)\n",
    "print(ytrain)\n",
    "# print(yvalid)\n",
    "\n",
    "# lreg = LogisticRegression()\n",
    "# lreg.fit(xtrain_bow, ytrain)    \n",
    "# train using training dataset and label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf feature matrix\n",
    "train_tfidf = tfidf[:31962, :]\n",
    "test_tfidf = tfidf[31962:, :]\n",
    "\n",
    "\n",
    "# ytrain.index index of axis labels \n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "# extract label vectors from the feature matrix via matching index\n",
    "\n",
    "# print(xtrain_tfidf)\n",
    "\n",
    "\n",
    "# fit on logistic regression\n",
    "# lreg.fit(xtrain_tfidf, ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM tutorial: https://www.youtube.com/watch?v=FB5EdxAGxQg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(xtrain_bow, ytrain)        # Build using bow\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain_bow, ytrain)                     # training the model\n",
    "\n",
    "# predicting on the validation set\n",
    "prediction = lreg.predict_proba(xvalid_bow)\n",
    "# if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "prediction_int = prediction[:, 1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)  # calculating f1 score\n",
    "\n",
    "\n",
    "\n",
    "# accuracy test\n",
    "# svm.score(xvalid_bow, yvalid)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lreg.predict_proba(test_bow)\n",
    "test_pred_int = test_pred[:, 1] >= 0.3\n",
    "test_pred_int = test_pred_int.astype(np.int)\n",
    "\n",
    "# If any texts are classified as '1'\n",
    "# Then the website contains hate speech to some extent\n",
    "if 1 in test_pred_int:\n",
    "    print(\"It is racist\")\n",
    "test['label'] = test_pred_int\n",
    "submission = test[['id', 'label']]\n",
    "# writing data to a CSV file\n",
    "submission.to_csv('predict_lreg_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(xtrain_tfidf, ytrain)      # Build using TD-IDF\n",
    "\n",
    "\n",
    "svm.score(xvalid_tfidf, yvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict(test_tfidf)\n",
    "\n",
    "# if '1' in svm.predict(test_tfidf):\n",
    "#     print(\"it is racist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm.predict(xvalid_bow)\n",
    "prediction_int = prediction.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
