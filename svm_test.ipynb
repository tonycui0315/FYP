{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech Filtering\n",
    "\n",
    "Using Tf-Idf and BoW to build SVM model\n",
    "\n",
    "Linear model? Or RBF model?\n",
    "\n",
    "RACIST texts classified with '1'\n",
    "NON-RACIST texts classified with '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Logistic Regression\n",
    "lreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data pandas, keep raw copy of data\n",
    "train_RAW = pd.read_csv('train.csv')\n",
    "test_RAW = pd.read_csv('test.csv')\n",
    "train = train_RAW\n",
    "test = test_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "Ref: \n",
    "    https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20(term%20frequency%2D,across%20a%20set%20of%20documents\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF freq\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words is a Natural Language Processing technique of text modelling. \n",
    "In technical terms, we can say that it is a method of feature extraction with text data.\n",
    "\n",
    "Ref: \n",
    "    https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words Model\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "Ref/Doc:\n",
    "    https://scikit-learn.org/stable/modules/svm.html#:~:text=Support%20vector%20machines%20(SVMs)%20are,than%20the%20number%20of%20samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/sry46qbs59n25vtlyr3vfqrh0000gn/T/ipykernel_10421/2057826554.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  allTweets = train.append(test, ignore_index=True, sort=False)\n"
     ]
    }
   ],
   "source": [
    "# Combine training data with test data\n",
    "allTweets = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "# allTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc_clean_tweet(tweet_txt, pattern):\n",
    "    r = re.findall(pattern, tweet_txt)\n",
    "    for i in r:\n",
    "        tweet_txt = re.sub(i, '', tweet_txt)  # Substitute pattern with blank\n",
    "\n",
    "    return tweet_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/sry46qbs59n25vtlyr3vfqrh0000gn/T/ipykernel_10421/4278745381.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  allTweets['tidy_tweet'] = allTweets['tidy_tweet'].str.replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when a father is dysfunctional and is so sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for #lyft credit i can t use cause th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0    when a father is dysfunctional and is so sel...  \n",
       "1    thanks for #lyft credit i can t use cause th...  \n",
       "2                                bihday your majesty  \n",
       "3  #model   i love u take with u all the time in ...  \n",
       "4             factsguide  society now    #motivation  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text \n",
    "# Add new column 'tidy_tweet'\n",
    "\n",
    "# Clean user handles (@user_name)\n",
    "allTweets['tidy_tweet'] = np.vectorize(fnc_clean_tweet)(\n",
    "    allTweets['tweet'], \"@[\\w]*\") \n",
    "\n",
    "# Clean non-alphabets\n",
    "allTweets['tidy_tweet'] = allTweets['tidy_tweet'].str.replace(\n",
    "    \"[^a-zA-Z#]\", \" \") \n",
    "\n",
    "allTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [when, a, father, is, dysfunctional, and, is, ...\n",
       "1    [thanks, for, #lyft, credit, i, can, t, use, c...\n",
       "2                              [bihday, your, majesty]\n",
       "3    [#model, i, love, u, take, with, u, all, the, ...\n",
       "4              [factsguide, society, now, #motivation]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenized_tweet = allTweets['tidy_tweet'].apply(lambda x: x.split())\n",
    "\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming \n",
    "# # TODO: need to be worked on more, words not stemmed properly\n",
    "\n",
    "# tokenized_tweet = tokenized_tweet.apply(\n",
    "#     lambda x: [stemmer.stem(i) for i in x])  # stemming\n",
    "\n",
    "\n",
    "# # Combine tokens back together\n",
    "# for i in range(len(tokenized_tweet)):\n",
    "#     tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "# # Replace with stemmed \n",
    "# allTweets['tidy_tweet'] = tokenized_tweet\n",
    "\n",
    "\n",
    "# allTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/tonycui/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/tonycui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for #lyft credit i can t use cause they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model i love u take with u all the time in ur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  when a father is dysfunctional and is so selfi...  \n",
       "1  thanks for #lyft credit i can t use cause they...  \n",
       "2                                bihday your majesty  \n",
       "3     #model i love u take with u all the time in ur  \n",
       "4                 factsguide society now #motivation  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using lemmatizer instead of stemmer\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "tokenized_tweet = tokenized_tweet.apply(\n",
    "    lambda x: [lemmatizer.lemmatize(i) for i in x])  \n",
    "\n",
    "# Combine tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "# Replace with lemmatized\n",
    "allTweets['tidy_tweet'] = tokenized_tweet\n",
    "\n",
    "allTweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1877)\t1\n",
      "  (0, 4704)\t1\n",
      "  (0, 1542)\t1\n",
      "  (0, 2949)\t1\n",
      "  (0, 4579)\t1\n",
      "  (1, 5329)\t1\n",
      "  (1, 3261)\t1\n",
      "  (1, 1215)\t1\n",
      "  (1, 5624)\t1\n",
      "  (1, 844)\t1\n",
      "  (1, 1519)\t1\n",
      "  (1, 3748)\t1\n",
      "  (1, 5644)\t1\n",
      "  (1, 3927)\t1\n",
      "  (2, 513)\t1\n",
      "  (2, 3284)\t1\n",
      "  (3, 3487)\t1\n",
      "  (3, 3221)\t1\n",
      "  (3, 5394)\t1\n",
      "  (3, 5619)\t1\n",
      "  (4, 1822)\t1\n",
      "  (4, 4912)\t1\n",
      "  (4, 3528)\t1\n",
      "  (5, 2598)\t1\n",
      "  (5, 1850)\t1\n",
      "  :\t:\n",
      "  (49156, 4633)\t1\n",
      "  (49156, 3130)\t1\n",
      "  (49156, 5900)\t2\n",
      "  (49156, 1426)\t1\n",
      "  (49156, 2515)\t1\n",
      "  (49156, 5625)\t1\n",
      "  (49156, 989)\t1\n",
      "  (49156, 286)\t1\n",
      "  (49156, 3756)\t1\n",
      "  (49156, 5337)\t1\n",
      "  (49157, 2414)\t1\n",
      "  (49157, 5903)\t2\n",
      "  (49157, 1096)\t1\n",
      "  (49157, 4526)\t1\n",
      "  (49157, 3453)\t2\n",
      "  (49157, 3050)\t1\n",
      "  (49157, 1248)\t1\n",
      "  (49157, 1415)\t1\n",
      "  (49157, 3817)\t1\n",
      "  (49158, 2060)\t1\n",
      "  (49158, 3653)\t1\n",
      "  (49158, 4931)\t1\n",
      "  (49158, 1537)\t1\n",
      "  (49158, 2214)\t1\n",
      "  (49158, 3658)\t1\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction - Bag-of-Words [sklearn’s CountVectorizer]\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(allTweets['tidy_tweet'])\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4579)\t0.4024983119617675\n",
      "  (0, 2949)\t0.36455809535458955\n",
      "  (0, 1542)\t0.5730362603952134\n",
      "  (0, 4704)\t0.5410298170296455\n",
      "  (0, 1877)\t0.2898425181654669\n",
      "  (1, 3927)\t0.40479498927673024\n",
      "  (1, 5644)\t0.4003549376374816\n",
      "  (1, 3748)\t0.3325307874161632\n",
      "  (1, 1519)\t0.2057834060163094\n",
      "  (1, 844)\t0.29563720941752136\n",
      "  (1, 5624)\t0.28022184986744003\n",
      "  (1, 1215)\t0.3584978041999103\n",
      "  (1, 3261)\t0.4153857597592834\n",
      "  (1, 5329)\t0.23652009646579678\n",
      "  (2, 3284)\t0.8801390220492387\n",
      "  (2, 513)\t0.4747160223398928\n",
      "  (3, 5619)\t0.5647875413808863\n",
      "  (3, 5394)\t0.45621564623313093\n",
      "  (3, 3221)\t0.37161906008001283\n",
      "  (3, 3487)\t0.5786031380992227\n",
      "  (4, 3528)\t0.45998601499572633\n",
      "  (4, 4912)\t0.5851033747216161\n",
      "  (4, 1822)\t0.6678824049918725\n",
      "  (5, 3920)\t0.27416804146586254\n",
      "  (5, 888)\t0.44221772005280136\n",
      "  :\t:\n",
      "  (49156, 989)\t0.28164252510602494\n",
      "  (49156, 5625)\t0.251826461349804\n",
      "  (49156, 2515)\t0.2583320003151624\n",
      "  (49156, 1426)\t0.21414209466080109\n",
      "  (49156, 5900)\t0.43346666874721856\n",
      "  (49156, 3130)\t0.15622420919684082\n",
      "  (49156, 4633)\t0.19343242306474268\n",
      "  (49156, 169)\t0.2829165923498156\n",
      "  (49156, 3777)\t0.23568309782190777\n",
      "  (49156, 5407)\t0.15444122351855674\n",
      "  (49157, 3817)\t0.29720956033335033\n",
      "  (49157, 1415)\t0.2913572029368914\n",
      "  (49157, 1248)\t0.28275595345123306\n",
      "  (49157, 3050)\t0.2833147471795638\n",
      "  (49157, 3453)\t0.5774263422481054\n",
      "  (49157, 4526)\t0.22308022223203544\n",
      "  (49157, 1096)\t0.2782866711856963\n",
      "  (49157, 5903)\t0.4224734746852138\n",
      "  (49157, 2414)\t0.16566836727745393\n",
      "  (49158, 3658)\t0.506714335911647\n",
      "  (49158, 2214)\t0.38058191484592785\n",
      "  (49158, 1537)\t0.4202992490885622\n",
      "  (49158, 4931)\t0.33260024069425964\n",
      "  (49158, 3653)\t0.4705924208178253\n",
      "  (49158, 2060)\t0.2994434545091197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TF-IDF Features - Looks at frequency of occurence for terms/importance of the term\n",
    "\n",
    "# TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "# IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "# \n",
    "# TF-IDF = TF*IDF\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(allTweets['tidy_tweet'])\n",
    "\n",
    "print(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18711    0\n",
      "4487     0\n",
      "9172     0\n",
      "11563    0\n",
      "8731     0\n",
      "        ..\n",
      "7751     0\n",
      "17700    0\n",
      "1248     0\n",
      "22490    0\n",
      "16050    0\n",
      "Name: label, Length: 28765, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_bow = bow[:31962, :]\n",
    "test_bow = bow[31962:, :]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "\n",
    "\n",
    "\n",
    "# xtrain_bow training dataset\n",
    "# xvalid_bow validation for training\n",
    "# ytrain label vector\n",
    "# yvalid validation label vector\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(\n",
    "    train_bow, train['label'], test_size=0.1)\n",
    "\n",
    "# train_bow feature matrix\n",
    "# train['label'] label vector\n",
    "# random_state shuffles data before split into training and testing\n",
    "# test_size percentage of data gets tested on (0.9 training)\n",
    "\n",
    "\n",
    "# print(xtrain_bow)\n",
    "# print(xvalid_bow)\n",
    "print(ytrain)\n",
    "# print(yvalid)\n",
    "\n",
    "# lreg = LogisticRegression()\n",
    "# lreg.fit(xtrain_bow, ytrain)    \n",
    "# train using training dataset and label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf feature matrix\n",
    "train_tfidf = tfidf[:31962, :]\n",
    "test_tfidf = tfidf[31962:, :]\n",
    "\n",
    "\n",
    "# ytrain.index index of axis labels \n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "# extract label vectors from the feature matrix via matching index\n",
    "\n",
    "# print(xtrain_tfidf)\n",
    "\n",
    "\n",
    "# fit on logistic regression\n",
    "# lreg.fit(xtrain_tfidf, ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM tutorial: https://www.youtube.com/watch?v=FB5EdxAGxQg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9562089458867689"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: understand how svm model is built using BoW and TF-IDF\n",
    "\n",
    "svm.fit(xtrain_bow, ytrain)        # Build using bow\n",
    "# accuracy test\n",
    "svm.score(xvalid_bow, yvalid)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.960275258054426"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(xtrain_tfidf, ytrain)      # Build using TD-IDF\n",
    "svm.score(xvalid_tfidf, yvalid)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
