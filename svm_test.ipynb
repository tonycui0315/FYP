{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech Filtering\n",
    "\n",
    "Using Tf-Idf and BoW to build SVM model\n",
    "\n",
    "Linear model? Or RBF model?\n",
    "\n",
    "RACIST texts classified with '1'\n",
    "NON-RACIST texts classified with '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "import re \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Logistic Regression\n",
    "lreg = LogisticRegression()\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data pandas, keep raw copy of data\n",
    "train_RAW = pd.read_csv('train.csv')\n",
    "test_RAW = pd.read_csv('test.csv')\n",
    "train = train_RAW\n",
    "test = test_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "Ref: \n",
    "    https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20(term%20frequency%2D,across%20a%20set%20of%20documents\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF freq\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words is a Natural Language Processing technique of text modelling. \n",
    "In technical terms, we can say that it is a method of feature extraction with text data.\n",
    "\n",
    "Ref: \n",
    "    https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "\n",
    "\n",
    "Doc:\n",
    "    https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words Model\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_df=0.90, \n",
    "    min_df=2, \n",
    "    max_features=6000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "Ref/Doc:\n",
    "    https://scikit-learn.org/stable/modules/svm.html#:~:text=Support%20vector%20machines%20(SVMs)%20are,than%20the%20number%20of%20samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/sry46qbs59n25vtlyr3vfqrh0000gn/T/ipykernel_32898/1233045329.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  allTweets = train.append(test, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine training data with test data\n",
    "allTweets = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "allTweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnc_clean_tweet(tweet_txt, pattern):\n",
    "    r = re.findall(pattern, tweet_txt)\n",
    "    for i in r:\n",
    "        tweet_txt = re.sub(i, '', tweet_txt)  # Substitute pattern with blank\n",
    "\n",
    "    return tweet_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text \n",
    "# Add new column 'tidy_tweet'\n",
    "\n",
    "# Clean user handles (@user_name)\n",
    "allTweets['tidy_tweet'] = np.vectorize(fnc_clean_tweet)(\n",
    "    allTweets['tweet'], \"@[\\w]*\") \n",
    "\n",
    "# Clean non-alphabets\n",
    "allTweets['tidy_tweet'] = allTweets['tidy_tweet'].str.replace(\n",
    "    \"[^a-zA-Z#]\", \" \") \n",
    "\n",
    "allTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenized_tweet = allTweets['tidy_tweet'].apply(lambda x: x.split())\n",
    "\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming \n",
    "# # TODO: need to be worked on more, words not stemmed properly\n",
    "\n",
    "# tokenized_tweet = tokenized_tweet.apply(\n",
    "#     lambda x: [stemmer.stem(i) for i in x])  # stemming\n",
    "\n",
    "\n",
    "# # Combine tokens back together\n",
    "# for i in range(len(tokenized_tweet)):\n",
    "#     tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "# # Replace with stemmed \n",
    "# allTweets['tidy_tweet'] = tokenized_tweet\n",
    "\n",
    "\n",
    "# allTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lemmatizer instead of stemmer\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "tokenized_tweet = tokenized_tweet.apply(\n",
    "    lambda x: [lemmatizer.lemmatize(i) for i in x])  \n",
    "\n",
    "# Combine tokens back together\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "# Replace with lemmatized\n",
    "allTweets['tidy_tweet'] = tokenized_tweet\n",
    "\n",
    "allTweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction - Bag-of-Words [sklearnâ€™s CountVectorizer]\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(allTweets['tidy_tweet'])\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Features - Looks at frequency of occurence for terms/importance of the term\n",
    "\n",
    "# TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "# IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "# \n",
    "# TF-IDF = TF*IDF\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(allTweets['tidy_tweet'])\n",
    "\n",
    "print(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:31962, :]\n",
    "test_bow = bow[31962:, :]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "\n",
    "\n",
    "\n",
    "# xtrain_bow training dataset\n",
    "# xvalid_bow validation for training\n",
    "# ytrain label vector\n",
    "# yvalid validation label vector\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(\n",
    "    train_bow, train['label'], test_size=0.1)\n",
    "\n",
    "# train_bow feature matrix\n",
    "# train['label'] label vector\n",
    "# random_state shuffles data before split into training and testing\n",
    "# test_size percentage of data gets tested on (0.9 training)\n",
    "\n",
    "\n",
    "# print(xtrain_bow)\n",
    "# print(xvalid_bow)\n",
    "print(ytrain)\n",
    "# print(yvalid)\n",
    "\n",
    "# lreg = LogisticRegression()\n",
    "# lreg.fit(xtrain_bow, ytrain)    \n",
    "# train using training dataset and label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf feature matrix\n",
    "train_tfidf = tfidf[:31962, :]\n",
    "test_tfidf = tfidf[31962:, :]\n",
    "\n",
    "\n",
    "# ytrain.index index of axis labels \n",
    "xtrain_tfidf = train_tfidf[ytrain.index]\n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "# extract label vectors from the feature matrix via matching index\n",
    "\n",
    "# print(xtrain_tfidf)\n",
    "\n",
    "\n",
    "# fit on logistic regression\n",
    "# lreg.fit(xtrain_tfidf, ytrain) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM tutorial: https://www.youtube.com/watch?v=FB5EdxAGxQg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: understand how svm model is built using BoW and TF-IDF\n",
    "\n",
    "svm.fit(xtrain_bow, ytrain)        # Build using bow\n",
    "# accuracy test\n",
    "svm.score(xvalid_bow, yvalid)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(xtrain_tfidf, ytrain)      # Build using TD-IDF\n",
    "# svm.predict()\n",
    "\n",
    "svm.score(xvalid_tfidf, yvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 343)\t0.29188034974046045\n",
      "  (0, 631)\t0.33999969477857217\n",
      "  (0, 2422)\t0.4204121982725948\n",
      "  (0, 3520)\t0.24578186116628126\n",
      "  (0, 4109)\t0.31876934796752654\n",
      "  (0, 4292)\t0.3135049280930342\n",
      "  (0, 5387)\t0.30282436814586644\n",
      "  (0, 5392)\t0.3137986577923866\n",
      "  (0, 5527)\t0.3341725991303641\n",
      "  (0, 5732)\t0.2455772190657296\n",
      "  (1, 505)\t0.5843222589842813\n",
      "  (1, 3003)\t0.5163961965879316\n",
      "  (1, 3754)\t0.5431965840627105\n",
      "  (1, 5394)\t0.3111911580807328\n",
      "  (2, 827)\t0.4014224757585932\n",
      "  (2, 2539)\t0.32214155957109086\n",
      "  (2, 2627)\t0.41877109411793484\n",
      "  (2, 3858)\t0.4392838032267027\n",
      "  (2, 3905)\t0.48976569022567723\n",
      "  (2, 4735)\t0.35619501847255536\n",
      "  (3, 445)\t0.47997076441169745\n",
      "  (3, 3998)\t0.7246307675279473\n",
      "  (3, 5348)\t0.49450815570818346\n",
      "  (4, 481)\t0.5485282804215882\n",
      "  (4, 1313)\t0.4812945015641103\n",
      "  :\t:\n",
      "  (28761, 2971)\t0.4387384483244518\n",
      "  (28761, 3645)\t0.24888291766138218\n",
      "  (28761, 4019)\t0.5174577923851978\n",
      "  (28761, 4320)\t0.5003931349250015\n",
      "  (28761, 4623)\t0.4768752571459091\n",
      "  (28762, 781)\t0.3570239743490468\n",
      "  (28762, 874)\t0.3918884615716093\n",
      "  (28762, 1250)\t0.44120097141159126\n",
      "  (28762, 3154)\t0.34033602211399283\n",
      "  (28762, 3731)\t0.38419324193323146\n",
      "  (28762, 3737)\t0.4247087601830652\n",
      "  (28762, 5873)\t0.2837048328469883\n",
      "  (28763, 1238)\t0.5050321823330033\n",
      "  (28763, 2105)\t0.3334332143690065\n",
      "  (28763, 2849)\t0.45931477954128996\n",
      "  (28763, 3777)\t0.3648072496313583\n",
      "  (28763, 4496)\t0.36545808671237584\n",
      "  (28763, 5245)\t0.3951591794851613\n",
      "  (28764, 431)\t0.2368107012243037\n",
      "  (28764, 1177)\t0.2537316778798275\n",
      "  (28764, 1700)\t0.22165589067285787\n",
      "  (28764, 2011)\t0.6411778946680771\n",
      "  (28764, 2017)\t0.24113318449816265\n",
      "  (28764, 4075)\t0.2931416244116103\n",
      "  (28764, 4415)\t0.5246172665058529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/sry46qbs59n25vtlyr3vfqrh0000gn/T/ipykernel_32898/845311533.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  prediction_int = prediction_int.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.652054794520548"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.figure(figsize=(16,5))\n",
    "# plt.plot(xvalid_tfidf, yvalid)\n",
    "\n",
    "lreg.fit(xtrain_tfidf, ytrain)                   # Train model\n",
    "\n",
    "print(xtrain_tfidf)\n",
    "\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)    # Predict validation set\n",
    "prediction_int = prediction[:,1] >= 0.3\n",
    "prediction_int = prediction_int.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, prediction_int)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
