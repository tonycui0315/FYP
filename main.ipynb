{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hate Speech Filttering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and instances\n",
    "\n",
    "# Web scraper\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "# NLTK\n",
    "import re \n",
    "import csv\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "logReg = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# run Chrome tab without interface\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "unvisited = set()\n",
    "visited = set()\n",
    "baseUrl = input(\"Enter url of website: \")\n",
    "unvisited.add(baseUrl)\n",
    "\n",
    "def getLink(pageDriver, baseUrl):\n",
    "    # Get all 'a-tags'\n",
    "    allTags = driver.find_elements_by_tag_name('a')\n",
    "\n",
    "    # Get all allUrls on page\n",
    "    allUrls = [tag.get_attribute('href') for tag in allTags]\n",
    "\n",
    "    #Only base url and all related allUrls\n",
    "    useful_urls = [url for url in allUrls if url and baseUrl in url]\n",
    "\n",
    "    return useful_urls\n",
    "\n",
    "def getText(pageDriver):\n",
    "    # Get textual content from 'body' html tag\n",
    "    pageText = pageDriver.find_element_by_xpath(\"/html/body\")\n",
    "    if not pageText:\n",
    "        return \"\"\n",
    "    # pageText is webElement, pageText.text returns texts\n",
    "    return pageText.text\n",
    "\n",
    "# counter for text indexing\n",
    "ct = 31963\n",
    "while unvisited:\n",
    "    # Pop url from unvisited url set and add it to visited\n",
    "    page = unvisited.pop()\n",
    "    visited.add(page)\n",
    "\n",
    "    # Get page content\n",
    "    driver.get(page)\n",
    "\n",
    "    # Get links\n",
    "    links = getLink(driver, baseUrl)\n",
    "\n",
    "    # Get text\n",
    "    text = getText(driver)\n",
    "    textToWrite = text.splitlines()\n",
    "    # Create .csv file\n",
    "    with open(f'test.csv', 'w', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['id', 'text']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        # Write to .csv file with index and text\n",
    "        for text in textToWrite:\n",
    "            writer.writerow({'id': ct, 'text': {text}})\n",
    "            ct += 1\n",
    "\n",
    "print(\"WEB SCRAPING FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw .csv data using Pandas Dataframe\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training data with test data\n",
    "allTexts = train.append(test, ignore_index=True, sort=False)\n",
    "\n",
    "allTexts[31980:31990]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(text, pattern):\n",
    "    process = re.findall(pattern, text)\n",
    "    for i in process:\n",
    "        text = re.sub(i, '', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text \n",
    "# Add new column 'tidy_text'\n",
    "# Clean usernames with '@' symbols\n",
    "allTexts['tidy_text'] = np.vectorize(clean_texts) (allTexts['text'], \"@[\\w]*\") \n",
    "\n",
    "# Clean non-alphabets\n",
    "allTexts['tidy_text'] = allTexts['tidy_text'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "\n",
    "allTexts[31990:32000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine word type\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "\n",
    "    pos_counts = Counter()\n",
    "    # noun\n",
    "    pos_counts[\"n\"] = len([item for item in w_synsets if item.pos() == \"n\"])\n",
    "    # verb\n",
    "    pos_counts[\"v\"] = len([item for item in w_synsets if item.pos() == \"v\"])\n",
    "    # adj\n",
    "    pos_counts[\"a\"] = len([item for item in w_synsets if item.pos() == \"a\"])\n",
    "    # adv\n",
    "    pos_counts[\"r\"] = len([item for item in w_synsets if item.pos() == \"r\"])\n",
    "\n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "# Tokenization and Lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, get_pos(w)) for w in tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Lemmatization text\n",
    "nltkText = allTexts['tidy_text'].apply(lemmatize_text)\n",
    "# Join back \n",
    "allTexts['tidy_text'] = [' '.join(map(str, l)) for l in nltkText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag Of Words Model\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_df=0.5, \n",
    "    min_df=2, \n",
    "    max_features=5000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction - Bag-of-Words [sklearn CountVectorizer] \n",
    "# Matrix dimensions change accordingly to test data size\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(allTexts['tidy_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature matrix into training and testing matrices\n",
    "trainingBow = bow[:31962, :]\n",
    "testingBow = bow[31962:, :]\n",
    "\n",
    "# splitting data into training and validation set\n",
    "# xtrainBow training dataset\n",
    "# xtrainValidBow validation for training\n",
    "# ytrain training label vector\n",
    "# yvalid validation label vector\n",
    "xtrainBow, xtrainValidBow, ytrain, yvalid = train_test_split(trainingBow, train['label'], test_size=0.1)\n",
    "# trainingBow feature matrix\n",
    "# train['label'] label vector\n",
    "# test_size percentage of data gets tested on (0.9 training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW LR \n",
    "logReg.fit(xtrainBow, ytrain)                     \n",
    "\n",
    "# predicting on the validation set\n",
    "predValid = logReg.predict_proba(xtrainValidBow)\n",
    "# if prediction probability is greater than or equal to 0.25 than 1 else 0\n",
    "predValidNum = predValid[:, 1] >= 0.25\n",
    "predValidNum = predValidNum.astype(np.int)\n",
    "# calculating f1 score\n",
    "f1_score(yvalid, predValidNum)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on testing dataset Bow LR\n",
    "predTest = logReg.predict_proba(testingBow)\n",
    "predTestNum = predTest[:, 1] >= 0.25\n",
    "predTestNum = predTestNum.astype(np.int)\n",
    "test['predicted_label'] = predTestNum\n",
    "sub = test[['id', 'predicted_label']]\n",
    "# writing data to a CSV file\n",
    "sub.to_csv('logReg_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW SVM\n",
    "svm = SVC(kernel='rbf', gamma=0.1, C=11)\n",
    "svm.fit(xtrainBow, ytrain)\n",
    "# Prediction on validating dataset Bow SVM\n",
    "predValid = svm.predict(xtrainValidBow)\n",
    "predValidNum = predValid.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, predValidNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on testing dataset Bow SVM\n",
    "predTest = svm.predict(testingBow)\n",
    "predTestNum = predTest.astype(np.int)\n",
    "test['predicted_label'] = predTestNum\n",
    "sub = test[['id', 'predicted_label']]\n",
    "sub.to_csv('svm_bow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, \n",
    "    min_df=2, \n",
    "    max_features=5000, \n",
    "    stop_words='english'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Features - Looks at frequency of occurence for terms/importance of the term\n",
    "# TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "# IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "# TF-IDF = TF*IDF\n",
    "tfidf = tfidf_vectorizer.fit_transform(allTexts['tidy_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF feature matrix\n",
    "trainTFIDF = tfidf[:31962, :]\n",
    "testTFIDF = tfidf[31962:, :]\n",
    "# extract label vectors from the feature matrix via matching data index\n",
    "xtrainTFIDF = trainTFIDF[ytrain.index]\n",
    "xvalidTFIDF = trainTFIDF[yvalid.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF LR\n",
    "logReg.fit(xtrainTFIDF, ytrain) \n",
    "# Prediction on validating dataset TF-IDF LR\n",
    "predValid = logReg.predict_proba(xvalidTFIDF) \n",
    "predValidNum = predValid[:, 1] >= 0.25\n",
    "predValidNum = predValidNum.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, predValidNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on testing dataset TF-IDF LR\n",
    "predTest = logReg.predict_proba(testTFIDF)\n",
    "predTestNum = predTest[:, 1] >= 0.20\n",
    "predTestNum = predTestNum.astype(np.int)\n",
    "test['predicted-label'] = predTestNum\n",
    "sub = test[['id', 'predicted-label']]\n",
    "sub.to_csv('logReg_td-idf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF SVM\n",
    "svm = SVC(kernel='rbf', gamma=0.1, C=11)\n",
    "svm.fit(xtrainTFIDF, ytrain) \n",
    "# Prediction on validating dataset TF-IDF SVM\n",
    "predValid = svm.predict(xvalidTFIDF)\n",
    "predValidNum = predValid.astype(np.int)\n",
    "\n",
    "f1_score(yvalid, predValidNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on testing dataset TF-IDF SVM\n",
    "predTest = svm.predict(testTFIDF)\n",
    "predTestNum = predTest.astype(np.int)\n",
    "test['predicted-label'] = predTestNum\n",
    "sub = test[['id', 'predicted-label']]\n",
    "sub.to_csv('svm_td-idf.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
